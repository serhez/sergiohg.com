{"generatedAt":1713101924369,"generateTime":27,"contents":[{"_path":"/emojis","_dir":"","_draft":false,"_partial":false,"_locale":"","page":"üìÑ","pin":"üìç","canarias_flag":"üáÆüá®","spain_flag":"üá™üá∏","usa_flag":"üá∫üá∏","england_flag":"üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø","scotland_flag":"üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø","finland_flag":"üá´üáÆ","_id":"content:emojis.json","_type":"json","title":"Emojis","_source":"content","_file":"emojis.json","_extension":"json"},{"_path":"/navigation","_dir":"","_draft":false,"_partial":false,"_locale":"","body":[{"hidden":false,"href":"/","name":"About","external":false},{"hidden":false,"href":"/research","name":"Research","external":false},{"hidden":false,"href":"/projects","name":"Projects","external":false},{"hidden":false,"href":"/resume","name":"Resume","external":false}],"_id":"content:navigation.json","_type":"json","title":"Navigation","_source":"content","_file":"navigation.json","_extension":"json"},{"_path":"/projects","_dir":"","_draft":false,"_partial":false,"_locale":"","body":[{"title":"MLoggers","description":"A package of loggers well-suited for machine learning experiments. It integrates with Weights and Biases and provides structured file and console logging.","links":[{"title":"PyPI","url":"https://pypi.org/project/mloggers/","icon":"mdi:package-variant-closed"},{"title":"GitHub","url":"https://github.com/serhez/mloggers","icon":"mdi:github"}]},{"title":"LData","description":"A library for managing datasets and benchmarks in NLP experiments. It provides a simple API for using datasets to train and evaluate models across different pre-defined or user-defined language tasks.","links":[{"title":"GitHub","url":"https://github.com/serhez/ldata","icon":"mdi:github"}]},{"title":"LModels","description":"A library defining a common API to work with state-of-the-art LLMs. It provides wrappers for the Hugging Face and OpenAI APIs, amongst others.","links":[{"title":"GitHub","url":"https://github.com/serhez/lmodels","icon":"mdi:github"}]},{"title":"LMethods","description":"A library containing methods to enhance the capabilities of LLMs, such as Chain-of-Thought, Tree of Thoughts, Decomposed Prompting, etc. A common interface can be used to implement new methods.","links":[{"title":"GitHub","url":"https://github.com/serhez/lmethods","icon":"mdi:github"}]}],"_id":"content:projects.json","_type":"json","title":"Projects","_source":"content","_file":"projects.json","_extension":"json"},{"_path":"/research/publications","_dir":"research","_draft":false,"_partial":false,"_locale":"","body":[{"title":"Co-Adaptation of Agent Morphology and Behaviour with Self-Imitating Reinforcement Learning","authors":["Sergio Hern√°ndez-Guti√©rrez","Ville Kyrki","Kevin S. Luck"],"venue":"ICML","year":"2024","status":"submitted","abstract":"In this paper we consider the problem of co-adapting the body and behaviour of agents, a long-standing research problem in the community of evolutionary robotics. Previous work has largely focused on the development of methods exploiting massive parallelization of agent evaluations with large population sizes, a paradigm which is not applicable to the real world. More recent data-efficient approaches utilizing reinforcement learning can suffer from distributional shifts to transition dynamics as well as to states and action spaces when experiencing new body morphologies. In this work, we propose a new co-adaptation method combining reinforcement learning and State-Aligned Self-Imitation Learning. We show that the integration of a self-imitation signal improves data-efficiency, behavioural recovery for unseen designs and performance convergence.","links":[{"title":"arXiv","url":"https://arxiv.org/abs/1912.09363","icon":"mdi:file-document-outline"},{"title":"GitHub","url":"https://github.com/serhez/cosil","icon":"mdi:github"}]},{"title":"A Comprehensive Overview of Goal-Conditioned Hierarchical Reinforcement Learning: Algorithms, Challenges, and Future Directions","authors":["Sergio Hern√°ndez-Guti√©rrez","Vivienne Wang"],"venue":"Seminar work","year":"2023","status":"unpublished","abstract":"Hierarchical reinforcement learning (HRL) methods have recently enabled higher sample efficiency in high-dimensional and long reinforcement learning (RL) problems. Goal-conditioned HRL (GCHRL) approaches concretize these hierarchical ideas by providing reachable sub-goals and considering a chain of policies that model the actions required to reach them, which are either less abstract sub-goals or the agent's native actions. This paper analyses and compares the current state-of-the-art GCHRL methods. Additionally, it discusses the current and future key challenges of the area, including efficient state space exploration, meaningful sub-goal generation and representation, the non-stationarity of policies and the transfer of skills learnt for one problem to solve another. Finally, it contributes to the current discussion on future directions and key focus points within the field of GCHRL.","links":[{"title":"arXiv","url":"https://arxiv.org/abs/1912.09363","icon":"mdi:file-document-outline"}]},{"title":"3D Reconstruction of Fire-Damaged Parchments","authors":["Sergio Hern√°ndez-Guti√©rrez","Wanyue Zhang","Ionut Deocanu"],"venue":"Microsoft Blog, UK Faculty Connection","year":"2018","status":"published","abstract":"In this post in partnership with Microsoft, as a Microsoft Student Partner, I give an introduction to 3D reconstruction of physical objects. In particular, I explain the process of reconstructing fire-damaged parchments and, as part of my 2nd year project at UCL, building a product for archivists and other professionals who are in need of a parchment-reconstruction tool to read them.","links":[{"title":"Microsoft Blog","url":"https://learn.microsoft.com/en-us/archive/blogs/uk_faculty_connection/3d-reconstruction-of-fire-damage-parchments","icon":"mdi-microsoft"}]}],"_id":"content:research:publications.json","_type":"json","title":"Publications","_source":"content","_file":"research/publications.json","_extension":"json"},{"_path":"/socials","_dir":"","_draft":false,"_partial":false,"_locale":"","github":{"title":"Github","icon":"mdi:github","url":"https://github.com/serhez","username":"serhez"},"linkedin":{"title":"LinkedIn","icon":"mdi:linkedin","url":"https://www.linkedin.com/in/serhez/","username":"serhez"},"email":{"title":"email","icon":"mdi:email","url":"mailto:contact.sergiohernandez@gmail.com"},"_id":"content:socials.json","_type":"json","title":"Socials","_source":"content","_file":"socials.json","_extension":"json"}],"navigation":[{"title":"Emojis","_path":"/emojis"},{"title":"Navigation","_path":"/navigation"},{"title":"Projects","_path":"/projects"},{"title":"Research","_path":"/research","children":[{"title":"Publications","_path":"/research/publications"}]},{"title":"Socials","_path":"/socials"}]}